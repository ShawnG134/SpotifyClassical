{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a72fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def generate_fictional_piece_names(num_pieces=100000):\n",
    "    genres = [\"Concerto\", \"Symphony\", \"Sonata\", \"Suite\", \"Overture\", \"Opera\", \"Quartet\", \"Trio\", \"Cantata\", \"Etude\", \"Prelude\", \"Rhapsody\", \"Serenade\", \"Nocturne\"]\n",
    "    instruments = [\"Piano\", \"Violin\", \"Cello\", \"Flute\", \"Clarinet\", \"Harp\", \"Guitar\", \"Oboe\", \"Bassoon\", \"Horn\", \"Trumpet\", \"Trombone\"]\n",
    "    keys = [\"C Major\", \"G Major\", \"D Major\", \"A Major\", \"E Major\", \"B Major\", \"F Major\", \"B-Flat Major\", \"E-Flat Major\", \"A-Flat Major\", \"D-Flat Major\", \"G-Flat Major\", \n",
    "            \"C Minor\", \"G Minor\", \"D Minor\", \"A Minor\", \"E Minor\", \"B Minor\", \"F Minor\", \"B-Flat Minor\", \"E-Flat Minor\", \"A-Flat Minor\", \"D-Flat Minor\", \"G-Flat Minor\"]\n",
    "    opus_numbers = [f\"Op. {i}\" for i in range(1, 201)]\n",
    "    k_numbers = [f\"K. {i}\" for i in range(1, 501)]\n",
    "    \n",
    "    piece_names = []\n",
    "    \n",
    "    for _ in range(num_pieces):\n",
    "        genre = random.choice(genres)\n",
    "        instrument = random.choice(instruments)\n",
    "        number = random.randint(1, 30)\n",
    "        key = random.choice(keys)\n",
    "        opus = random.choice(opus_numbers)\n",
    "        k_number = random.choice(k_numbers)\n",
    "        \n",
    "        piece_name = f\"{instrument} {genre} No. {number} in {key}, {opus}, {k_number}\"\n",
    "        piece_names.append(piece_name)\n",
    "    \n",
    "    return piece_names\n",
    "\n",
    "def generate_variations(piece_name):\n",
    "    parts = piece_name.split(\", \")\n",
    "    title = parts[0]\n",
    "    details = parts[1:] if len(parts) > 1 else []\n",
    "    \n",
    "    variations = set()\n",
    "    variations.add(piece_name)  # Original full name\n",
    "\n",
    "    if len(details) == 2:\n",
    "        # Drop either opus or K number, not both\n",
    "        variations.add(f\"{title}, {details[1]}\")  # Drop opus number\n",
    "        variations.add(f\"{title}, {details[0]}\")  # Drop K number\n",
    "        variations.add(f\"{title} ({details[0]})\")  # Title with opus number in brackets\n",
    "        variations.add(f\"{details[0]}, {title}\")  # Opus number first, then title\n",
    "        variations.add(f\"{details[1]}, {title}\")  # K number first, then title\n",
    "        variations.add(f\"{title}\")  # Only title\n",
    "    elif len(details) == 1:\n",
    "        variations.add(f\"{details[0]}, {title}\")  # Reorder\n",
    "        variations.add(f\"{title}\")  # Only title\n",
    "        variations.add(f\"{details[0]}\")  # Only details part\n",
    "\n",
    "    return list(variations)\n",
    "\n",
    "def generate_csv(piece_names, filename=\"classical_pieces.csv\"):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Piece Name 1\", \"Piece Name 2\"])\n",
    "        count = 0\n",
    "        while count < 200000:\n",
    "            piece_name = random.choice(piece_names)\n",
    "            variations = generate_variations(piece_name)\n",
    "            if len(variations) > 1:\n",
    "                piece_pair = random.sample(variations, 2)\n",
    "                # Ensure neither of the names is just an opus or K number alone\n",
    "                if not (piece_pair[0].startswith(\"Op.\") or piece_pair[0].startswith(\"K.\")) and not (piece_pair[1].startswith(\"Op.\") or piece_pair[1].startswith(\"K.\")):\n",
    "                    writer.writerow(piece_pair)\n",
    "                    count += 1\n",
    "\n",
    "piece_names = generate_fictional_piece_names()\n",
    "generate_csv(piece_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26589df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def generate_fictional_piece_names(num_pieces=100000):\n",
    "    genres = [\"Concerto\", \"Symphony\", \"Sonata\", \"Suite\", \"Overture\", \"Opera\", \"Quartet\", \"Trio\", \"Cantata\", \"Etude\", \"Prelude\", \"Rhapsody\", \"Serenade\", \"Nocturne\"]\n",
    "    instruments = [\"Piano\", \"Violin\", \"Cello\", \"Flute\", \"Clarinet\", \"Harp\", \"Guitar\", \"Oboe\", \"Bassoon\", \"Horn\", \"Trumpet\", \"Trombone\"]\n",
    "    keys = [\"C Major\", \"G Major\", \"D Major\", \"A Major\", \"E Major\", \"B Major\", \"F Major\", \"B-Flat Major\", \"E-Flat Major\", \"A-Flat Major\", \"D-Flat Major\", \"G-Flat Major\", \n",
    "            \"C Minor\", \"G Minor\", \"D Minor\", \"A Minor\", \"E Minor\", \"B Minor\", \"F Minor\", \"B-Flat Minor\", \"E-Flat Minor\", \"A-Flat Minor\", \"D-Flat Minor\", \"G-Flat Minor\"]\n",
    "    opus_numbers = [f\"Op. {i}\" for i in range(1, 201)]\n",
    "    k_numbers = [f\"K. {i}\" for i in range(1, 501)]\n",
    "    \n",
    "    piece_names = []\n",
    "    \n",
    "    for _ in range(num_pieces):\n",
    "        genre = random.choice(genres)\n",
    "        instrument = random.choice(instruments)\n",
    "        number = random.randint(1, 30)\n",
    "        key = random.choice(keys)\n",
    "        opus = random.choice(opus_numbers)\n",
    "        k_number = random.choice(k_numbers)\n",
    "        \n",
    "        piece_name = f\"{instrument} {genre} No. {number} in {key}, {opus}, {k_number}\"\n",
    "        piece_names.append(piece_name)\n",
    "    \n",
    "    return piece_names\n",
    "\n",
    "def generate_similar_but_different_pairs(piece_names, num_pairs=200000):\n",
    "    keys = [\"C Major\", \"G Major\", \"D Major\", \"A Major\", \"E Major\", \"B Major\", \"F Major\", \"B-Flat Major\", \"E-Flat Major\", \"A-Flat Major\", \"D-Flat Major\", \"G-Flat Major\", \n",
    "            \"C Minor\", \"G Minor\", \"D Minor\", \"A Minor\", \"E Minor\", \"B Minor\", \"F Minor\", \"B-Flat Minor\", \"E-Flat Minor\", \"A-Flat Minor\", \"D-Flat Minor\", \"G-Flat Minor\"]\n",
    "    opus_numbers = [f\"Op. {i}\" for i in range(1, 201)]\n",
    "    k_numbers = [f\"K. {i}\" for i in range(1, 501)]\n",
    "    \n",
    "    pairs = []\n",
    "    for _ in range(num_pairs):\n",
    "        piece_name = random.choice(piece_names)\n",
    "        \n",
    "        # Split the piece name into components\n",
    "        parts = piece_name.split(\", \")\n",
    "        title_parts = parts[0].split(\" \")\n",
    "        \n",
    "        # Ensure changing the number after the genre\n",
    "        number_index = title_parts.index(\"No.\") + 1  # Index of the number after \"No.\"\n",
    "        new_number = random.randint(1, 30)\n",
    "        while new_number == int(title_parts[number_index]):\n",
    "            new_number = random.randint(1, 30)\n",
    "        title_parts[number_index] = str(new_number)\n",
    "\n",
    "        # Optionally, make additional changes to ensure diversity\n",
    "        change_attribute = random.choice([\"key\", \"opus\", \"k_number\"])\n",
    "        \n",
    "        if change_attribute == \"key\":\n",
    "            key_index = title_parts.index(\"in\") + 1  # Index of the key after \"in\"\n",
    "            new_key = random.choice(keys)\n",
    "            while new_key == \" \".join(title_parts[key_index:key_index + 2]):\n",
    "                new_key = random.choice(keys)\n",
    "            title_parts[key_index:key_index + 2] = new_key.split()\n",
    "        \n",
    "        elif change_attribute == \"opus\":\n",
    "            new_opus = random.choice(opus_numbers)\n",
    "            while new_opus == parts[1]:\n",
    "                new_opus = random.choice(opus_numbers)\n",
    "            parts[1] = new_opus\n",
    "        \n",
    "        elif change_attribute == \"k_number\":\n",
    "            new_k_number = random.choice(k_numbers)\n",
    "            while new_k_number == parts[2]:\n",
    "                new_k_number = random.choice(k_numbers)\n",
    "            parts[2] = new_k_number\n",
    "        \n",
    "        # Reconstruct the similar but different piece name\n",
    "        similar_piece_name = \" \".join(title_parts)\n",
    "        similar_piece_full_name = f\"{similar_piece_name}, {parts[1]}, {parts[2]}\"\n",
    "        \n",
    "        pairs.append((piece_name, similar_piece_full_name))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def generate_csv(pairs, filename=\"different_classical_pieces.csv\"):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Piece Name 1\", \"Piece Name 2\"])\n",
    "        writer.writerows(pairs)\n",
    "\n",
    "# Generate the piece names\n",
    "piece_names = generate_fictional_piece_names()\n",
    "\n",
    "# Generate the pairs of similar but different pieces\n",
    "pairs = generate_similar_but_different_pairs(piece_names)\n",
    "\n",
    "# Save to CSV\n",
    "generate_csv(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76631ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "csv1 = pd.read_csv('classical_pieces.csv')\n",
    "csv2 = pd.read_csv('different_classical_pieces.csv')\n",
    "\n",
    "# Append a new column with value 1 to the first CSV\n",
    "csv1['new_column'] = 1\n",
    "\n",
    "# Append a new column with value 0 to the second CSV\n",
    "csv2['new_column'] = 0\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "merged_csv = pd.concat([csv1, csv2], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_csv.to_csv('path_to_merged_csv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03ba248",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a524a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Combine both columns of strings into a single dataframe\n",
    "X = df[['Piece Name 1', 'Piece Name 2']]\n",
    "y = df['new_column']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_combined = X_train['Piece Name 1'] + \" \" + X_train['Piece Name 2']\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_combined)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_combined = X_test['Piece Name 1'] + \" \" + X_test['Piece Name 2']\n",
    "X_test_tfidf = vectorizer.transform(X_test_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f85bf330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9931\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     39949\n",
      "           1       0.99      1.00      0.99     40051\n",
      "\n",
      "    accuracy                           0.99     80000\n",
      "   macro avg       0.99      0.99      0.99     80000\n",
      "weighted avg       0.99      0.99      0.99     80000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc1bab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/seangong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Combine all text data for training Word2Vec\n",
    "all_text = pd.concat([df['Piece Name 1'], df['Piece Name 2']])\n",
    "\n",
    "# Tokenize the text\n",
    "nltk.download('punkt')\n",
    "tokenized_text = [word_tokenize(text.lower()) for text in all_text]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model for later use\n",
    "word2vec_model.save(\"word2vec_model.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b6c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sentence_vector(sentence, model):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Apply the function to both columns\n",
    "X1 = np.array([get_sentence_vector(text, word2vec_model) for text in df['Piece Name 1']])\n",
    "X2 = np.array([get_sentence_vector(text, word2vec_model) for text in df['Piece Name 2']])\n",
    "\n",
    "# Combine the vectors (e.g., concatenation, difference, or other methods)\n",
    "X = np.concatenate([X1, X2], axis=1)\n",
    "\n",
    "y = df['new_column']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9ac05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seangong/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39949\n",
      "           1       1.00      1.00      1.00     40051\n",
      "\n",
      "    accuracy                           1.00     80000\n",
      "   macro avg       1.00      1.00      1.00     80000\n",
      "weighted avg       1.00      1.00      1.00     80000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39949     0]\n",
      " [    0 40051]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the base classifier\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Initialize the AdaBoost classifier\n",
    "adaboost_classifier = AdaBoostClassifier(base_estimator=base_classifier, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bae5750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_vector(sentence, model):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Example strings to predict\n",
    "string1 = \"Piano Concerto 1\"\n",
    "string2 = \"\"\n",
    "\n",
    "# Convert the strings into vectors\n",
    "vector1 = get_sentence_vector(string1, word2vec_model)\n",
    "vector2 = get_sentence_vector(string2, word2vec_model)\n",
    "\n",
    "# Combine the vectors (same method as used during training)\n",
    "combined_vector = np.concatenate([vector1, vector2])\n",
    "\n",
    "# Predict the label\n",
    "prediction = adaboost_classifier.predict([combined_vector])\n",
    "\n",
    "print(f\"Prediction: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db65614a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
